{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "question_embeddings is a tensor that contains the embeddings for each question (queries).\n",
    "answer_embeddings is a tensor that contains the embeddings for each answer (corpus).\n",
    "scores_q_a is a score that measures the relevance between a question and its corresponding answer. This score can be manually labeled, or derived by using a more powerful embedding model, which will serve as the ground truth.\n",
    "\n",
    "'''\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size)  \n",
    "        # self.batch_norm1 = nn.BatchNorm1d(input_size)  \n",
    "        # self.relu = nn.ReLU()  \n",
    "        # self.fc2 = nn.Linear(input_size, input_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.fc1(x)\n",
    "        # x = self.batch_norm1(x)  \n",
    "        # x = self.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        return x +  z\n",
    "         \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size)  \n",
    "        # self.batch_norm1 = nn.BatchNorm1d(input_size)  \n",
    "        # self.relu = nn.ReLU()  \n",
    "        # self.fc2 = nn.Linear(input_size, input_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.fc1(x)\n",
    "        # x = self.batch_norm1(x)  \n",
    "        # x = self.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        return x + z\n",
    "        \n",
    "class Models(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Models, self).__init__()\n",
    "        self.encoder = encoder(input_size)\n",
    "        self.decoder = decoder(input_size)\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    def decode(self,x):\n",
    "        return self.decoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_indices = []\n",
    "for k  in range(len(scores_q_a)) :\n",
    "    non_zero =  torch.nonzero(scores_q_a[k], as_tuple=True)[0]\n",
    "    non_zero_indices.append( non_zero if non_zero.numel() > 0 else None )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, question_embeddings, answer_embeddings, scores, answer_batch_size, non_zero_indices, device):\n",
    "        super().__init__()  \n",
    "        self.question_embeddings = question_embeddings\n",
    "        self.answer_embeddings = answer_embeddings  \n",
    "        self.scores = scores\n",
    "        self.answer_batch_size = answer_batch_size\n",
    "        self.non_zero_indices = non_zero_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.question_embeddings.size(0)  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        relevant_indices = self.non_zero_indices[index]\n",
    "\n",
    "        if relevant_indices is not None:  \n",
    "            num_relevant = min(self.answer_batch_size//2, relevant_indices.numel())\n",
    "            num_other = self.answer_batch_size - num_relevant\n",
    "\n",
    "            all_indices = torch.randperm(relevant_indices.numel())  \n",
    "            random_other_indices = all_indices[:num_other].to(device)\n",
    "            random_relevant_indices = relevant_indices[torch.randperm(relevant_indices.size(0))[:num_relevant]].to(device)\n",
    "            # while torch.any(random_relevant_indices[:, None] == random_other_indices): That will take longer !!\n",
    "            #     random_relevant_indices = torch.randint(0, relevant_indices.size(0), size=(self.answer_batch_size - self.answer_batch_size // 2,))\n",
    "\n",
    "            corpus_indices = torch.cat([random_relevant_indices, random_other_indices])\n",
    "        else:\n",
    "             corpus_indices = torch.randint(0, self.answer_embeddings.size(0), size=(self.answer_batch_size,))\n",
    "        corpus_indices = torch.randint(0, self.answer_embeddings.size(0), size=(self.answer_batch_size,))\n",
    "\n",
    "        return index, corpus_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = answer_embeddings_.shape[1]\n",
    "models_dummy = Models(input_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "def ranking_loss(criterion_recovery,criterion_pred, question_batch,their_answers_batch, answer_batch , scores_batch, question_encoded,their_answers_encoded , answer_encoded , answer_decoded , alpha, beta) :\n",
    "    diff = (scores_batch.unsqueeze(1) - scores_batch.unsqueeze(2))\n",
    "    similarity_encoded_question_answer = torch.sum(F.normalize(question_encoded, dim = 1).unsqueeze(1)*F.normalize(answer_encoded, dim = 2) , dim = 2)\n",
    "\n",
    "    sim = torch.log(1+ (similarity_encoded_question_answer.unsqueeze(1)-similarity_encoded_question_answer.unsqueeze(2)))\n",
    "    loss_rank = torch.sum((diff > 0)*diff*sim )\n",
    "    loss = loss_rank\n",
    "    loss_recovery = alpha*(criterion_recovery(question_batch, question_encoded) + criterion_recovery(their_answers_batch, their_answers_encoded) )\n",
    "  \n",
    "    decoding_contribution = torch.norm(question_encoded.unsqueeze(1)- answer_decoded, p=1 , dim =  2 )  # (N , M)  \n",
    "\n",
    "    \n",
    "    loss_pred = beta*(scores_batch* decoding_contribution / scores_batch.sum()) # scores (N , M) *  decoding_part (N, M) ........\n",
    "    return loss, loss_rank, loss_recovery, loss_pred\n",
    "\n",
    "\n",
    "criterion_recovery = nn.L1Loss()\n",
    "criterion_pred = nn.L1Loss(reduction=  'none')\n",
    "\n",
    "batch_size = 64\n",
    "dataset = CustomDataset(question_embeddings, answer_embeddings , scores_q_a , 4*batch_size, non_zero_indices , device)\n",
    "dataloader = DataLoader(dataset, batch_size= batch_size , shuffle= True)\n",
    "\n",
    "optimizer= torch.optim.AdamW(models_dummy.parameters(), lr=0.01)  # type: ignore\n",
    "\n",
    "alpha = 0\n",
    "beta= 0\n",
    "total_train_loss = 0\n",
    "for  k ,batch in enumerate(dataloader) :\n",
    "    batch_indices, corpus_indices = batch\n",
    "   \n",
    "    question_batch  = question_embeddings[batch_indices]\n",
    "    their_answers_batch = answer_embeddings[batch_indices]\n",
    "    # answer_batch , scores_batch = answer_embeddings[batch_indices], \n",
    "\n",
    "    scores_batch = scores_q_a[batch_indices.unsqueeze(1), corpus_indices]\n",
    "    answer_batch =  answer_embeddings[corpus_indices]  # (N, M , dim)\n",
    "    M = answer_batch.size(1)\n",
    "    N = answer_batch.size(0)\n",
    "    \n",
    "    answer_batch_flattened = answer_batch .view(M*N,-1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    question_encoded= models_dummy(question_batch)\n",
    "    answer_encoded_flattened = models_dummy(answer_batch_flattened)\n",
    "    their_answers_encoded = models_dummy(their_answers_batch)\n",
    "    answer_decoded_flattened = models_dummy.decode(answer_batch_flattened)\n",
    "\n",
    "    answer_encoded = answer_encoded_flattened.view((N, M , input_dim))\n",
    "    answer_decoded = answer_decoded_flattened.view((N, M , input_dim))\n",
    "    \n",
    "\n",
    "    loss,_,_,_ = ranking_loss(criterion_recovery,criterion_pred, question_batch,their_answers_batch, answer_batch , scores_batch, question_encoded,their_answers_encoded , answer_encoded , answer_decoded , alpha, beta) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_train_loss += loss.item()\n",
    "\n",
    "avg_train_loss = total_train_loss/len(dataloader) \n",
    "\n",
    "print(f'Train Loss: {avg_train_loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(writer, models, train_dataset, valid_dataset, question_embeddings, ground_truth, num_train, num_epochs, batch_size, learning_rate , tau, alpha, beta, hparams, k_values,path,patience, commentary, factor):    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \n",
    "    # valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "    device_ = torch.device('cpu')\n",
    "    k_max = max(k_values) \n",
    "    print('We ended the dataloader part .')\n",
    "\n",
    "    optimizer= torch.optim.AdamW(models.parameters(), lr=learning_rate)  # type: ignore\n",
    "    # early_stopping = EarlyStopping(patience=patience , path = path)\n",
    "    evaluating  = 0\n",
    "    # Loss functions :\n",
    "    criterion_recovery = nn.L1Loss()\n",
    "    criterion_pred = nn.L1Loss(reduce = False)\n",
    "    # Initialize TensorBoard writer\n",
    "    writer.add_text('Run Commentary', commentary, 0)\n",
    "\n",
    "    # Log hyperparameters\n",
    "    writer.add_hparams(hparams, {})\n",
    "    Step = 0 \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'We started the process of epoch {epoch + 1}')\n",
    "        models.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_reg = 0\n",
    "        total_train_triplets = 0\n",
    "        for batch in train_loader:\n",
    "            batch_indices = batch[0]\n",
    "            question_batch , answer_batch , scores_batch = question_embeddings[batch_indices],answer_embeddings[batch_indices], scores_q_a[batch_indices[:, None], batch_indices]\n",
    "            question_encoded, answer_encoded = models_dummy(question_batch), models_dummy(answer_batch)\n",
    "            answer_decoded = models_dummy.decode(answer_batch)\n",
    "\n",
    "            loss, loss_rank, loss_recovery, loss_pred  = ranking_loss(criterion_recovery =  criterion_recovery,criterion_pred = criterion_pred, question_batch = question_batch, answer_batch = answer_batch, scores = scores_batch, question_encoded = question_encoded, answer_encoded = answer_encoded , answer_decoded = answer_encoded ,alpha = alpha , beta = beta) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
