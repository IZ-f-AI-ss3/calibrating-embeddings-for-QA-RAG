{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "import os\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Use your prepared  dataset :\n",
    "answer_embeddings_ = np.array(data['answer_embedding'].tolist()).astype(np.float32)\n",
    "questions = data['question'].tolist()\n",
    "question_embeddings_ = np.array(data['question_embedding'].tolist()).astype(np.float32)\n",
    "answers = data['answer'].tolist()\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps')\n",
    "print(f\"Using {device} device\")\n",
    "dtype = torch.float32\n",
    "dtype_int = torch.int32\n",
    "\n",
    "answer_embeddings  = torch.tensor(answer_embeddings_ ,  device = device, dtype=dtype )\n",
    "question_embeddings  = torch.tensor(question_embeddings_ , device = device, dtype=dtype )\n",
    "\n",
    "eps = 0.8\n",
    "scores_q_a = torch.rand((answer_embeddings.size(0), answer_embeddings.size(0)), device = answer_embeddings.device)\n",
    "scores_q_a  = torch.where( scores_q_a < eps, 0 ,scores_q_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "question_embeddings is a tensor that contains the embeddings for each question (queries).\n",
    "answer_embeddings is a tensor that contains the embeddings for each answer (corpus).\n",
    "scores_q_a is a score that measures the relevance between a question and its corresponding answer. This score can be manually labeled, or derived by using a more powerful embedding model, which will serve as the ground truth.\n",
    "\n",
    "'''\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size//2)  \n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_size//2)  \n",
    "        self.relu = nn.ReLU()  \n",
    "        self.fc2 = nn.Linear(input_size//2, input_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.fc1(x)\n",
    "        z = self.batch_norm1(z)  \n",
    "        z = self.relu(z)\n",
    "        z = self.fc2(z)\n",
    "        return x + z\n",
    "         \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size//2)  \n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_size//2)  \n",
    "        self.relu = nn.ReLU()  \n",
    "        self.fc2 = nn.Linear(input_size//2, input_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.fc1(x)\n",
    "        z = self.batch_norm1(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.fc2(z)\n",
    "        return x + z\n",
    "        \n",
    "class Models(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Models, self).__init__()\n",
    "        self.encoder = encoder(input_size)\n",
    "        self.decoder = decoder(input_size)\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    def decode(self,x):\n",
    "        return self.decoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, question_embeddings, answer_embeddings, scores, answer_batch_size, device):\n",
    "        super().__init__()  \n",
    "        self.question_embeddings = question_embeddings\n",
    "        self.answer_embeddings = answer_embeddings  \n",
    "        self.scores = scores\n",
    "        self.answer_batch_size = answer_batch_size\n",
    "        non_zero_indices = []\n",
    "        for k  in range(len(scores_q_a)) :\n",
    "            non_zero =  torch.nonzero(scores_q_a[k], as_tuple=True)[0]\n",
    "            non_zero_indices.append( non_zero if non_zero.numel() > 0 else None )\n",
    "\n",
    "        self.non_zero_indices = non_zero_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.question_embeddings.size(0)  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        relevant_indices = self.non_zero_indices[index]\n",
    "\n",
    "        if relevant_indices is not None:  \n",
    "            num_relevant = min(self.answer_batch_size//2, relevant_indices.numel())\n",
    "            num_other = self.answer_batch_size - num_relevant\n",
    "\n",
    "            all_indices = torch.randperm(relevant_indices.numel())  \n",
    "            random_other_indices = all_indices[:num_other].to(device)\n",
    "            random_relevant_indices = relevant_indices[torch.randperm(relevant_indices.size(0))[:num_relevant]].to(device)\n",
    "            # while torch.any(random_relevant_indices[:, None] == random_other_indices): That will take longer !!\n",
    "            #     random_relevant_indices = torch.randint(0, relevant_indices.size(0), size=(self.answer_batch_size - self.answer_batch_size // 2,))\n",
    "\n",
    "            corpus_indices = torch.cat([random_relevant_indices, random_other_indices])\n",
    "        else:\n",
    "             corpus_indices = torch.randint(0, self.answer_embeddings.size(0), size=(self.answer_batch_size,))\n",
    "        corpus_indices = torch.randint(0, self.answer_embeddings.size(0), size=(self.answer_batch_size,))\n",
    "\n",
    "        return index, corpus_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = answer_embeddings_.shape[1]\n",
    "models_dummy = Models(input_dim).to(device)\n",
    "\n",
    "criterion_recovery = nn.L1Loss(reduction ='sum')\n",
    "criterion_pred = nn.L1Loss(reduction=  'none')\n",
    "\n",
    "batch_size = 128\n",
    "dataset = CustomDataset(question_embeddings, answer_embeddings , scores_q_a , 2*batch_size, device)\n",
    "dataloader = DataLoader(dataset, batch_size= batch_size , shuffle= True)\n",
    "\n",
    "optimizer= torch.optim.AdamW(models_dummy.parameters(), lr = 3e-4)  # type: ignore\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "alpha = 0.1\n",
    "beta =  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/5\n",
      "Epoch 1/5 - Train Loss: 56.1745,  Rank Loss: 25.6878 ,Pred Loss: 25.6388, Recovery Loss: 4.8480 \n",
      "Epoch 1/5 - Val Loss: 36.3511, Rank Loss: 25.7814 , Pred Loss: 6.6155, Recovery Loss: 3.9542 .\n",
      "Starting epoch 2/5\n",
      "Epoch 2/5 - Train Loss: 35.8685,  Rank Loss: 25.6665 ,Pred Loss: 6.4122, Recovery Loss: 3.7898 \n",
      "Epoch 2/5 - Val Loss: 34.9239, Rank Loss: 25.7135 , Pred Loss: 5.5890, Recovery Loss: 3.6214 .\n",
      "Starting epoch 3/5\n",
      "Epoch 3/5 - Train Loss: 34.1623,  Rank Loss: 25.6893 ,Pred Loss: 4.9568, Recovery Loss: 3.5162 \n",
      "Epoch 3/5 - Val Loss: 33.3551, Rank Loss: 25.3898 , Pred Loss: 4.6059, Recovery Loss: 3.3594 .\n",
      "Starting epoch 4/5\n",
      "Epoch 4/5 - Train Loss: 33.4007,  Rank Loss: 25.6929 ,Pred Loss: 4.3933, Recovery Loss: 3.3145 \n",
      "Epoch 4/5 - Val Loss: 33.6683, Rank Loss: 26.0893 , Pred Loss: 4.3935, Recovery Loss: 3.1855 .\n",
      "Starting epoch 5/5\n",
      "Epoch 5/5 - Train Loss: 32.8941,  Rank Loss: 25.6816 ,Pred Loss: 4.0474, Recovery Loss: 3.1652 \n",
      "Epoch 5/5 - Val Loss: 32.7191, Rank Loss: 25.7247 , Pred Loss: 3.9372, Recovery Loss: 3.0573 .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ranking_loss_final(criterion_recovery, question_batch, answer_sampled_batch,  scores_batch, question_batch_encoded, answer_sampled_encoded, answer_sampled_decoded, alpha, beta, M , N) :\n",
    "    diff = scores_batch.unsqueeze(1) - scores_batch.unsqueeze(2)  # (N, M, M), 1 -> j, 2 -> k\n",
    "    question_batch_encoded = F.normalize(question_batch_encoded, dim=1)  # (N, dim)\n",
    "    answer_sampled_encoded = F.normalize(answer_sampled_encoded, dim=2)  # (N, M, dim)\n",
    "    N, M = answer_sampled_encoded.size(0), answer_sampled_encoded.size(1)\n",
    "    similarity_encoded_question_answer = torch.sum(question_batch_encoded.unsqueeze(1) * answer_sampled_encoded, dim=2)  # (N, M)\n",
    "    sim = torch.log(1 + torch.exp(similarity_encoded_question_answer.unsqueeze(2) - similarity_encoded_question_answer.unsqueeze(1)))  # (N, M, M)\n",
    "    loss_rank = torch.sum(F.relu(diff) * sim) / (M * N) \n",
    "    loss_recovery_question = torch.sum(torch.norm(question_batch - question_batch_encoded, p=1, dim=1)) / N\n",
    "    loss_recovery_answer = torch.sum(torch.norm(answer_sampled_batch - answer_sampled_encoded, p=1, dim=2)) / (M * N)\n",
    "    loss_recovery = alpha * (loss_recovery_question + loss_recovery_answer) \n",
    "    decoding_contribution = torch.norm(question_batch_encoded.unsqueeze(1) - answer_sampled_decoded, p=1, dim=2)  # (N, M)\n",
    "    loss_pred = beta * torch.sum(scores_batch * decoding_contribution) / (M * N)  \n",
    "    loss = loss_rank + loss_recovery + loss_pred\n",
    "\n",
    "    return loss, loss_rank, loss_recovery, loss_pred\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "train_loader = dataloader\n",
    "val_loader = dataloader\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch + 1}/{num_epochs}')\n",
    "    \n",
    "    models_dummy.train()\n",
    "\n",
    "    total_train_loss = 0.0\n",
    "    total_train_recovery = 0.0\n",
    "    total_train_pred = 0.0\n",
    "    total_train_rank = 0.0\n",
    "    \n",
    "    for k, batch in enumerate(train_loader):\n",
    "        batch_indices, corpus_indices = batch\n",
    "        question_batch = question_embeddings[batch_indices]  # (N, dim)\n",
    "        scores_batch = scores_q_a[batch_indices.unsqueeze(1), corpus_indices]  # (N, M)\n",
    "        answers_sampled_batch = answer_embeddings[corpus_indices]  # (N, M, dim)\n",
    "        \n",
    "        N, M = answers_sampled_batch.size(0), answers_sampled_batch.size(1)\n",
    "        answer_sampled_batch_flattened = answers_sampled_batch.view(N * M, -1)  # (N*M, dim)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        question_batch_encoded = models_dummy(question_batch)  # (N, dim)\n",
    "        answer_sampled_encoded_flattened = models_dummy(answer_sampled_batch_flattened)  # (N*M, dim)\n",
    "        \n",
    "        answer_sampled_decoded_flattened = models_dummy.decode(answer_sampled_batch_flattened)  # (N*M, dim)\n",
    "        \n",
    "        answer_sampled_encoded = answer_sampled_encoded_flattened.view(N, M, input_dim)  # (N, M, dim)\n",
    "        answer_sampled_decoded = answer_sampled_decoded_flattened.view(N, M, input_dim)  # (N, M, dim)\n",
    "        \n",
    "        loss, loss_rank, loss_recovery, loss_pred = ranking_loss_final(\n",
    "            criterion_recovery=criterion_recovery,\n",
    "            question_batch=question_batch,\n",
    "            answer_sampled_batch=answers_sampled_batch,\n",
    "            scores_batch=scores_batch,\n",
    "            question_batch_encoded=question_batch_encoded,\n",
    "            answer_sampled_encoded=answer_sampled_encoded,\n",
    "            answer_sampled_decoded=answer_sampled_decoded,\n",
    "            alpha=alpha, beta=beta,\n",
    "            M = M,\n",
    "            N = N\n",
    "        )\n",
    "\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_pred += loss_pred.item()\n",
    "        total_train_recovery += loss_recovery.item()\n",
    "        total_train_rank += loss_rank.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    avg_train_pred = total_train_pred / len(dataloader)\n",
    "    avg_train_recovery = total_train_recovery / len(dataloader)\n",
    "    avg_train_rank = total_train_rank / len(dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, '\n",
    "          f' Rank Loss: {avg_train_rank:.4f} ,'\n",
    "          f'Pred Loss: {avg_train_pred:.4f}, Recovery Loss: {avg_train_recovery:.4f} ')\n",
    "\n",
    "    \n",
    "    writer.add_scalar('Loss/Train/Global', avg_train_loss, epoch*len(train_loader))\n",
    "    writer.add_scalar('Loss/Train/Pred', avg_train_pred,epoch*len(train_loader))\n",
    "    writer.add_scalar('Loss/Train/Recovery', avg_train_recovery,epoch*len(train_loader))\n",
    "    writer.add_scalar('Loss/Train/Rank', avg_train_rank,epoch*len(train_loader))\n",
    "    \n",
    "\n",
    "    # Validation step\n",
    "    models_dummy.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_recovery = 0.0\n",
    "    total_val_pred = 0.0\n",
    "    total_val_rank = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            batch_indices, corpus_indices = batch\n",
    "            question_batch = question_embeddings[batch_indices]  # (N, dim)\n",
    "            scores_batch = scores_q_a[batch_indices.unsqueeze(1), corpus_indices]  # (N, M)\n",
    "            answers_sampled_batch = answer_embeddings[corpus_indices]  # (N, M, dim)\n",
    "            N, M = answers_sampled_batch.size(0), answers_sampled_batch.size(1)\n",
    "            answer_sampled_batch_flattened = answers_sampled_batch.view(N * M, -1)  # (N*M, dim)\n",
    "            question_batch_encoded = models_dummy(question_batch)  # (N, dim)\n",
    "            answer_sampled_encoded_flattened = models_dummy(answer_sampled_batch_flattened)  # (N*M, dim)\n",
    "            \n",
    "            answer_sampled_decoded_flattened = models_dummy.decode(answer_sampled_batch_flattened)  # (N*M, dim)\n",
    "            \n",
    "            answer_sampled_encoded = answer_sampled_encoded_flattened.view(N, M, input_dim)  # (N, M, dim)\n",
    "            answer_sampled_decoded = answer_sampled_decoded_flattened.view(N, M, input_dim)  # (N, M, dim)\n",
    "            \n",
    "            loss, loss_rank, loss_recovery, loss_pred = ranking_loss_final(\n",
    "                criterion_recovery=criterion_recovery,\n",
    "                question_batch=question_batch,\n",
    "                answer_sampled_batch=answers_sampled_batch,\n",
    "                scores_batch=scores_batch,\n",
    "                question_batch_encoded=question_batch_encoded,\n",
    "                answer_sampled_encoded=answer_sampled_encoded,\n",
    "                answer_sampled_decoded=answer_sampled_decoded,\n",
    "                alpha=alpha, beta=beta,\n",
    "                M = M,\n",
    "                N = N\n",
    "            )\n",
    "\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            total_val_pred += loss_pred.item()\n",
    "            total_val_recovery += loss_recovery.item()\n",
    "            total_val_rank += loss_rank.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    avg_val_pred = total_val_pred / len(val_loader)\n",
    "    avg_val_recovery = total_val_recovery / len(val_loader)\n",
    "    avg_val_rank = total_val_rank / len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Val Loss: {avg_val_loss:.4f}, '\n",
    "          f'Rank Loss: {avg_val_rank:.4f} , '\n",
    "          f'Pred Loss: {avg_val_pred:.4f}, Recovery Loss: {avg_val_recovery:.4f} .')\n",
    "    \n",
    "   \n",
    "    writer.add_scalar('Loss/Valid/Global', avg_train_loss, epoch*len(train_loader))\n",
    "    writer.add_scalar('Loss/Valid/Pred', avg_train_pred,epoch*len(train_loader))\n",
    "    writer.add_scalar('Loss/Valid/Recovery', avg_train_recovery,epoch*len(train_loader))\n",
    "    writer.add_scalar('Loss/Valid/Rank', avg_train_rank,epoch*len(train_loader))\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no evaluation using @NDCG, as was done in the article, because the scores here were sampled randomly. Although a dataset with relevant scores wasnâ€™t available, I did my best with this code.\n",
    "\n",
    "This is the function I will be using if everything was provided ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaluation_metrics(true_data, predicted_current, predicted_old, log_writer, top_k_values, iteration_step):\n",
    "    def compute_dcg(relevance_vals):\n",
    "        dcg_score = 0.0\n",
    "        for idx, rel in enumerate(relevance_vals):\n",
    "            dcg_score += rel / np.log2(idx + 2)\n",
    "        return dcg_score\n",
    "\n",
    "    def normalized_dcg_at_top_k(actual_ids, pred_ids, top_k):\n",
    "        ndcg_scores = []\n",
    "        for actual, predicted in zip(actual_ids, pred_ids):\n",
    "            relevance_scores = [1 if item in actual else 0 for item in set(predicted[:top_k])]\n",
    "            dcg_val = compute_dcg(relevance_scores)\n",
    "            ideal_relevance = sorted(relevance_scores, reverse=True)\n",
    "            idcg_val = compute_dcg(ideal_relevance)\n",
    "            ndcg = dcg_val / idcg_val if idcg_val else 0\n",
    "            ndcg_scores.append(ndcg)\n",
    "        return sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0\n",
    "\n",
    "    # Model comparison and logging function\n",
    "    def log_and_compare(true_ids, current_model_ids, baseline_model_ids, top_k_values, step):\n",
    "        for  top_k in top_k_values:\n",
    "           \n",
    "            baseline_ndcg = normalized_dcg_at_top_k(true_ids, baseline_model_ids, top_k)\n",
    "            current_ndcg = normalized_dcg_at_top_k(true_ids, current_model_ids, top_k)\n",
    "            log_writer.add_scalar(f'NDCG@{top_k}/Baseline', baseline_ndcg, step)\n",
    "            log_writer.add_scalar(f'NDCG@{top_k}/Current', current_ndcg, step)\n",
    "\n",
    "    actual_relevant_ids, baseline_model_ids, current_model_ids = true_data, predicted_old, predicted_current\n",
    "    primary_evaluation = log_and_compare(actual_relevant_ids, current_model_ids, baseline_model_ids, top_k_values, iteration_step)\n",
    "    return primary_evaluation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
